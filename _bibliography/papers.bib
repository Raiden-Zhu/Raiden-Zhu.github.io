---
---

@InProceedings{pmlr-v162-zhu22d,
  abbr={ICML 2022 (Spotlight)},
  title = 	 {Topology-aware Generalization of Decentralized {SGD}},
  author =       {Zhu, Tongtian and He, Fengxiang and Zhang, Lan and Niu, Zhengyang and Song, Mingli and Tao, Dacheng},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {27479--27503},
  year = 	 {2022},
  month = 	 {17--23 Jul},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
  pdf = 	 {https://arxiv.org/pdf/2206.12680},
  url = 	 {https://proceedings.mlr.press/v162/zhu22d.html},
  abstract = 	 {This paper studies the algorithmic stability and generalizability of decentralized stochastic gradient descent (D-SGD). We prove that the consensus model learned by D-SGD is \mathcal{O}{(N^{-1}+ m^{-1} +\lambda^2)}\mathcal{O}{(N^{-1}+ m^{-1} +\lambda^2)}-stable in expectation in the non-convex non-smooth setting, where NN is the total sample size, mm is the worker number, and 1+\lambda1+\lambda is the spectral gap that measures the connectivity of the communication topology. These results then deliver an \mathcal{O}{(N^{-(1+\alpha)/2}+ m^{-(1+\alpha)/2}+\lambda^{1+\alpha} + \phi_S)}\mathcal{O}{(N^{-(1+\alpha)/2}+ m^{-(1+\alpha)/2}+\lambda^{1+\alpha} + \phi_S)} in-average generalization bound, which is non-vacuous even when \lambda\lambda is closed to 11, in contrast to vacuous as suggested by existing literature on the projected version of D-SGD. Our theory indicates that the generalizability of D-SGD is positively correlated with the spectral gap, and can explain why consensus control in initial training phase can ensure better generalization. Experiments of VGG-11 and ResNet-18 on CIFAR-10, CIFAR-100 and Tiny-ImageNet justify our theory. To our best knowledge, this is the first work on the topology-aware generalization of vanilla D-SGD.},
  tldr =  {<strong><span style="font-weight: bold; color: #B509AC;">One-sentence summary</span>: The first work on the topology-aware generalization analysis of vanilla Decentralized SGD algorithm.</strong>},
  slides = {https://github.com/Raiden-Zhu/Generalization-of-DSGD/blob/main/Slides_ICML2022_Generalization_of_D_SGD.pdf},
  poster = {https://github.com/Raiden-Zhu/Generalization-of-DSGD/blob/main/Poster_ICML2022_Generalization_of_D_SGD_v8.png},
  code = {https://github.com/Raiden-Zhu/Generalization-of-DSGD}, 
  dimensions={true},
  selected={true},
  bibtex_show={true},
  preview={Topology.png}
}


@InProceedings{pmlr-zhu23,
  abbr={ICML 2023 (My Favorite)},
  title = 	 {Decentralized SGD and Average-direction SAM are Asymptotically Equivalent},
  author =       {Zhu, Tongtian and He, Fengxiang and Chen, Kaixuan and Song, Mingli and Tao, Dacheng},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  year = 	 {2023},
  month = 	 {23--29 Jul},
  pages = 	 {43005--43036},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
  pdf = 	 {https://arxiv.org/pdf/2306.02913},
  url = 	 {https://proceedings.mlr.press/v202/zhu23e.html},
  abstract = 	 {Decentralized stochastic gradient descent (D-SGD) allows collaborative learning on massive device simultaneously without the control of a central server. Sharpness-aware minimization, or SAM, is a popular optimization technique that effectively improves model generalization by explicitly minimizing a sharpness-based measure alongside the training loss. In this paper, we prove that D-SGD asymptotically minimizes the loss function of an average-direction SAM. This asymptotic equivalence further demonstrates three advantages of D-SGD: (1) D-SGD exhibits a gradient smoothing effect; (2) there exists an uncertainty self-estimating mechanism in D-SGD to improve posterior estimation; and (3) the sharpness regularization effect of D-SGD does not decrease as total batch size increases, which justifies the superiority of D-SGD over centralized SGD (C-SGD) in large-batch settings. We conduct extensive experiments which are in full agreement with our theory. Our code will be made publicly available.},
  tldr =  {<strong><span style="font-weight: bold; color: #B509AC;">TLDR</span>: The first work on the surprising sharpness-aware minimization nature (i.e., a kind of unique <span style="font-weight: bold; color: #B509AC;">implicit bias</span>) of decentralized learning. We provide a completely new perspective to understand model decentralization, which helps to bridge the gap between practice and exisiting theory in decentralized learning.</strong>},
  slides = {https://github.com/Raiden-Zhu/ICML-2023-DSGD-and-SAM/blob/main/Slides_ICML_2023_Decentralized_SGD_and_Average_direction_SAM_are_Asymptotically_Equivalent%20.pdf},
  poster = {https://github.com/Raiden-Zhu/ICML-2023-DSGD-and-SAM/blob/main/Poster_ICML2023_D_SGD_as_SAM.pdf},
  code = {https://github.com/Raiden-Zhu/ICML-2023-DSGD-and-SAM}, 
  dimensions={true},
  selected={true},
  bibtex_show={true},
  preview={230705 DSGD_SAM (2).png}
}

@inproceedings{liu2023,
  abbr={AAAI 2023 (Oral)},
  title={Contrastive Identity-Aware Learning for Multi-Agent Value Decomposition},
  author={Liu, Shunyu and Zhou, Yihe and Song, Jie and Zheng, Tongya and Chen, Kaixuan and Zhu, Tongtian and Feng, Zunlei and Song, Mingli},
  booktitle={Proceedings of the 37th AAAI Conference on Artificial Intelligence},
  year={2023},
  abstract = {Value Decomposition (VD) aims to deduce the contributions of agents for decentralized policies in the presence of only global rewards, and has recently emerged as a powerful credit assignment paradigm for tackling cooperative Multi-Agent Reinforcement Learning (MARL) problems. One of the main challenges in VD is to promote diverse behaviors among agents, while existing methods directly encourage the diversity of learned agent networks with various strategies. However, we argue that these dedicated designs for agent networks are still limited by the indistinguishable VD network, leading to homogeneous agent behaviors and thus downgrading the cooperation capability. In this paper, we propose a novel Contrastive Identity-Aware learning (CIA) method, explicitly boosting the credit-level distinguishability of the VD network to break the bottleneck of multi-agent diversity. Specifically, our approach leverages contrastive learning to maximize the mutual information between the temporal credits and identity representations of different agents, encouraging the full expressiveness of credit assignment and further the emergence of individualities. The algorithm implementation of the proposed CIA module is simple yet effective that can be readily incorporated into various VD architectures. Experiments on the SMAC benchmarks and across different VD backbones demonstrate that the proposed method yields results superior to the state-of-the-art counterparts. Our code is available at this https URL.},
  pdf = 	 {https://arxiv.org/pdf/2211.12712},
  code = {https://github.com/liushunyu/CIA}, 
  dimensions={true},
  bibtex_show={true},
  preview={contrastive_aware.png}
}


@inproceedings{10.1145/3580305.3599388,
  abbr={KDD 2023},
  author = {Chen, Kaixuan and Liu, Shunyu and Zhu, Tongtian and Qiao, Ji and Su, Yun and Tian, Yingjie and Zheng, Tongya and Zhang, Haofei and Feng, Zunlei and Ye, Jingwen and Song, Mingli},
  title = {Improving Expressivity of GNNs with Subgraph-Specific Factor Embedded Normalization},
  year = {2023},
  publisher = {Association for Computing Machinery},
  abstract = {Graph Neural Networks~(GNNs) have emerged as a powerful category of learning architecture for handling graph-structured data. However, existing GNNs typically ignore crucial structural characteristics in node-induced subgraphs, which thus limits their expressiveness for various downstream tasks. In this paper, we strive to strengthen the representative capabilities of GNNs by devising a dedicated plug-and-play normalization scheme, termed as SUbgraph-sPEcific FactoR Embedded Normalization (SuperNorm), that explicitly considers the intra-connection information within each node-induced subgraph. To this end, we embed the subgraph-specific factor at the beginning and the end of the standard BatchNorm, as well as incorporate graph instance-specific statistics for improved distinguishable capabilities. In the meantime, we provide theoretical analysis to support that, with the elaborated SuperNorm, an arbitrary GNN is at least as powerful as the 1-WL test in distinguishing non-isomorphism graphs. Furthermore, the proposed SuperNorm scheme is also demonstrated to alleviate the over-smoothing phenomenon. Experimental results related to predictions of graph, node, and link properties on the eight popular datasets demonstrate the effectiveness of the proposed method. The code is available at https://github.com/chenchkx/SuperNorm.},
  booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages = {237â€“249},
  numpages = {13},
  keywords = {graph normalization, subgraph-specific factor, graph isomorphism test, oversmoothing issue, graph neural networks},
  location = {Long Beach, CA, USA},
  series = {KDD '23},
  dimensions={true},
  bibtex_show={true},
  preview={KDD.png},
  code = {https://github.com/chenchkx/SuperNorm} 
}

@InProceedings{wang2023adversarial,
  abbr={ECAI 2023},
  title = 	 {Adversarial Erasing with Pruned Elements: Towards Better Graph Lottery Ticket},
  author =       {Wang, Yuwen and Liu, Shunyu and Chen, Kaixuan and Zhu, Tongtian and Qiao, Ji and Shi, Mengjie and Wan, Yuanyu and Song, Mingli},
  booktitle = 	 {European Conference on Artificial Intelligence},
  year = 	 {2023},
  month = 	 {30 Sep--04 Oct},
  series = 	 {Proceedings of European Conference on Artificial Intelligence},
  publisher =    {ECAI},
  pdf = 	 {https://arxiv.org/abs/2308.02916},
  abstract = 	 {Graph Lottery Ticket (GLT), a combination of core subgraph and sparse subnetwork, has been proposed to mitigate the computational cost of deep Graph Neural Networks (GNNs) on large input graphs while preserving original performance. However, the winning GLTs in exisiting studies are obtained by applying iterative magnitude-based pruning (IMP) without re-evaluating and re-considering the pruned information, which disregards the dynamic changes in the significance of edges/weights during graph/model structure pruning, and thus limits the appeal of the winning tickets. In this paper, we formulate a conjecture, i.e., existing overlooked valuable information in the pruned graph connections and model parameters which can be re-grouped into GLT to enhance the final performance. Specifically, we propose an adversarial complementary erasing (ACE) framework to explore the valuable information from the pruned components, thereby developing a more powerful GLT, referred to as the ACE-GLT. The main idea is to mine valuable information from pruned edges/weights after each round of IMP, and employ the ACE technique to refine the GLT processing. Finally, experimental results demonstrate that our ACE-GLT outperforms existing methods for searching GLT in diverse tasks.},
  code = {https://github.com/Wangyuwen0627/ACE-GLT}, 
  dimensions={true},
  bibtex_show={true},
  preview={2023ecai-wang2023adversarial.png}
}

@InProceedings{jiang2024lie,
  abbr={Preprint 2024},
  title={Lie Symmetry Net: Preserving Conservation Laws in Modelling Financial Market Dynamics via Differential Equations},
  author={Jiang, Xuelian and Zhu, Tongtian and Wang, Can and Xu, Yingxiang and He, Fengxiang},
  booktitle = {arXiv preprint arXiv:2406.09189},
  abstract = 	 {This paper introduce Lie symmetry net, a symmetry-aware approach that addresses a fundamental challenge in AI-driven SDE solvers: ensuring AI models can learn and preserve intrinsic symmetries from data. By incorporating Lie symmetry principles, LSN achieves a significant reduction in test errorâ€”over an order of magnitudeâ€”compared to state-of-the-art AI-driven methods. The framework is not limited to specific equations or methods but provides a universal solution that can be applied across various AI-driven differential equation solvers.},
  year={2024},
  dimensions={true},
  bibtex_show={true},
  code = {https://github.com/Jxl163/LSN_code},
  preview={LSN.jpg}
}

@InProceedings{openreview-dice25,
  abbr={ICLR 2025},
  title = {DICE: Data Influence Cascade in Decentralized Learning},
  author = {Zhu, Tongtian and Li, Wenhao and Wang, Can and He, Fengxiang},
  booktitle = {International Conference on Learning Representations},
  year = {2025},
  url = {https://openreview.net/forum?id=2TIYkqieKw},
  pdf = {https://openreview.net/pdf?id=2TIYkqieKw},
  abstract = {Decentralized learning offers a promising approach to crowdsource computational workloads
  across geographically distributed compute interconnected through peer-to-peer networks,
  accommodating the exponentially increasing compute demands in the era of large models.
  However, the absence of proper incentives in locally connected decentralized networks poses
  significant risks of free riding and malicious behaviors.
  Data influence, which ensures fair attribution of data source contributions,
  holds great potential for establishing effective incentive mechanisms.
  Despite the importance, little effort has been made to analyze data influence
  in decentralized scenarios, due to non-trivial challenges arising from the
  distributed nature and the localized connections inherent in decentralized networks.
  To overcome this fundamental challenge, we propose DICE,
  the first framework to systematically define and estimate
  Data Influence CascadEs in decentralized environments.
  DICE establishes a new perspective on influence measurement, seamlessly integrating
  self-level and community-level contributions to capture how data influence cascades
  implicitly through networks via communication.
  Theoretically, the framework derives tractable approximations of influence cascades
  over arbitrary neighbor hops, uncovering for the first time that data influence
  in decentralized learning is shaped by a synergistic interplay of data,
  communication topology, and the curvature information of optimization landscapes.
  By bridging theoretical insights with practical applications,
  DICE lays the foundations for incentivized decentralized learning,
  including selecting suitable collaborators and identifying malicious behaviors.
  We envision DICE will catalyze the development of scalable, autonomous,
  and reciprocal decentralized learning ecosystems.},
  tldr = {<strong><span style="font-weight: bold; color: #B509AC;">TLDR</span>: We propose DICE, the first framework to systematically define
  and estimate Data Influence Cascades in decentralized environments.
  DICE introduces a novel perspective on influence measurement,
  integrating self-level and community-level contributions
  to capture data influence propagation via communication.</strong>},
  dimensions={true},
  selected={true},
  bibtex_show={true}
}
